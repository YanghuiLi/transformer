# Transformer

A network only contains attention mechanism without underlying recurrent network. It solved speed issue and bottleneck issue of RNNs.

It is gluon implement based on the article 'The Annotated transformer' from harvard nlp team.


# Usage

```
python ./python/transformer.py -s 6,2,3,4,5,3,3,1
```
