{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    print('embedding dim = {}'.format(d_k))\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    print('scores dim = {}'.format(scores.shape))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch * n_queries * embedding_dim\n",
    "query, key, value  = torch.tensor(np.ones((16, 5, 10))), \\\n",
    "torch.tensor(np.ones((16, 5, 10))), \\\n",
    "torch.tensor(np.ones((16, 5, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim = 10\n",
      "scores dim = torch.Size([16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "res = attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16, 5, 10]), torch.Size([16, 5, 5])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(torch.nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        print('query shape = {}'.format(query.shape))\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        print('query shape = {}'.format(query.shape))\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        print('x shape = {}'.format(x.shape))\n",
    "        print('self.attn shape = {}'.format(self.attn.shape))\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch * n_queries * embedding_dim\n",
    "n_batch = 16\n",
    "n_query = 10\n",
    "n_emb = 20\n",
    "\n",
    "mlh = MultiHeadedAttention(2, n_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query, key, value  = torch.tensor(np.ones((n_batch, n_query, n_emb))), \\\n",
    "torch.tensor(np.ones((n_batch, n_query, n_emb))), \\\n",
    "torch.tensor(np.ones((n_batch, n_query, n_emb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape = torch.Size([16, 10, 20])\n",
      "query shape = torch.Size([16, 2, 10, 10])\n",
      "embedding dim = 10\n",
      "scores dim = torch.Size([16, 2, 10, 10])\n",
      "x shape = torch.Size([16, 2, 10, 10])\n",
      "self.attn shape = torch.Size([16, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "res = mlh(query.float(), key.float(), value.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20]), torch.Size([10, 20])]\n"
     ]
    }
   ],
   "source": [
    "print(len(res))\n",
    "print([x.shape for x in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in Linear Layer in Torch and Dense in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=5, bias=True)\n",
      "tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.,  9.],\n",
      "         [10., 11., 12., 13., 14.]],\n",
      "\n",
      "        [[15., 16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29.]]])\n",
      "torch.Size([2, 3, 5])\n",
      "tensor([[[ 17.,  17.,  17.,  17.,  17.],\n",
      "         [ 42.,  42.,  42.,  42.,  42.],\n",
      "         [ 67.,  67.,  67.,  67.,  67.]],\n",
      "\n",
      "        [[ 92.,  92.,  92.,  92.,  92.],\n",
      "         [117., 117., 117., 117., 117.],\n",
      "         [142., 142., 142., 142., 142.]]], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor(np.array(range(30)).reshape((2, 3, 5))).float()\n",
    "#input = torch.tensor(np.array(range(10)).reshape((2, 5))).float()\n",
    "\n",
    "lin = torch.nn.Linear(5, 5)\n",
    "lin.weight.data = torch.tensor(np.ones((5,5))).float()\n",
    "lin.bias.data = torch.tensor(7).float()\n",
    "print(lin)\n",
    "out = lin(input)\n",
    "\n",
    "print(input)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[101. 101.]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a layer\n",
    "net = nn.Dense(2, in_units=100, use_bias=True)\n",
    "net.initialize()\n",
    "# Update the weights of a layer\n",
    "net.weight.set_data(nd.ones((2,100)))\n",
    "net.bias.set_data(nd.ones((2)))\n",
    "\n",
    "net(nd.ones((1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(5 -> 5, linear)\n",
      "\n",
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]\n",
      " [25. 26. 27. 28. 29.]]\n",
      "<NDArray 6x5 @cpu(0)>\n",
      "(6, 5)\n",
      "\n",
      "[[ 17.  17.  17.  17.  17.]\n",
      " [ 42.  42.  42.  42.  42.]\n",
      " [ 67.  67.  67.  67.  67.]\n",
      " [ 92.  92.  92.  92.  92.]\n",
      " [117. 117. 117. 117. 117.]\n",
      " [142. 142. 142. 142. 142.]]\n",
      "<NDArray 6x5 @cpu(0)>\n",
      "\n",
      "[[[ 17.  17.  17.  17.  17.]\n",
      "  [ 42.  42.  42.  42.  42.]\n",
      "  [ 67.  67.  67.  67.  67.]]\n",
      "\n",
      " [[ 92.  92.  92.  92.  92.]\n",
      "  [117. 117. 117. 117. 117.]\n",
      "  [142. 142. 142. 142. 142.]]]\n",
      "<NDArray 2x3x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "_in = nd.array(range(30)).reshape((2, 3, 5))\n",
    "#_in = nd.array(range(10)).reshape((2, 5))\n",
    "_in = _in.reshape(-1, 5)\n",
    "dense = nn.Dense(5, in_units = 5)\n",
    "dense.collect_params().initialize()\n",
    "dense.weight.set_data(nd.ones((5,5)))\n",
    "dense.bias.set_data(nd.ones((5))*7)\n",
    "\n",
    "print(dense)\n",
    "print(_in)\n",
    "out = dense(_in)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "\n",
    "out1 = out.reshape((2, 3, 5))\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -inf, -1.6094, -0.3567, -2.3026,    -inf],\n",
      "        [   -inf, -1.6094, -0.3567, -2.3026,    -inf],\n",
      "        [   -inf, -1.6094, -0.3567, -2.3026,    -inf]])\n"
     ]
    }
   ],
   "source": [
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "print(predict.log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.dim() => return dimension\n",
    "# torch.scatter_\n",
    "#  - torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23)\n",
    "#  - nd.scatter_nd(nd.array([1.23,1.23]), nd.array([[0,1],[2,3]]), (2,4))\n",
    "# Fill o\n",
    "#  - torch: true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "#  - mxnet: true_dist[mask, :] = 0\n",
    "\n",
    "class LabelSmoothing(torch.nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = torch.nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        print('true list ={}'.format(true_dist))\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        print('true list after fill ={}'.format(true_dist))\n",
    "        print('target = {}'.format(target))\n",
    "        print('target = {}'.format(target.data.unsqueeze(1)))\n",
    "        \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        print('arget.data.unsqueeze(1) = {}'.format(target.data.unsqueeze(1)))\n",
    "        print('confidence = {}'.format(self.confidence))\n",
    "        print('true list after scatter ={}'.format(true_dist))\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        print('true list after padding ={}'.format(true_dist))\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        print('target data= {}'.format(target.data))\n",
    "        print('mask = {}'.format(mask))\n",
    "        print('true list after mask ={}'.format(true_dist))\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            print('true list after masking ={}'.format(true_dist))\n",
    "            \n",
    "        self.true_dist = true_dist\n",
    "        print('x = {}'.format(x))\n",
    "        print('true list ={}'.format(true_dist))\n",
    "        print('loss = {}'.format(self.criterion(x, torch.tensor(true_dist, requires_grad=False))))\n",
    "        return self.criterion(x, torch.tensor(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing(5, 0, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true list =tensor([[0.0000, 0.2000, 0.7000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.7000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.7000, 0.1000, 0.0000]])\n",
      "true list after fill =tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
      "        [0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
      "        [0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
      "target = tensor([2, 1, 0])\n",
      "target = tensor([[2],\n",
      "        [1],\n",
      "        [0]])\n",
      "arget.data.unsqueeze(1) = tensor([[2],\n",
      "        [1],\n",
      "        [0]])\n",
      "confidence = 0.6\n",
      "true list after scatter =tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.1333, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
      "true list after padding =tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
      "target data= tensor([2, 1, 0])\n",
      "mask = tensor([[2]])\n",
      "true list after mask =tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
      "true list after masking =tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "x = tensor([[0.0000, 0.2000, 0.7000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.7000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.7000, 0.1000, 0.0000]])\n",
      "true list =tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
      "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "loss = -2.9115798473358154\n"
     ]
    }
   ],
   "source": [
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(predict, torch.LongTensor([2, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b74e7b6a0>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(torch.nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = torch.nn.KLDivLoss()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        #print('target = {}'.format(target.dim()))\n",
    "        if mask.dim() > 0 and list(mask.size())[0] > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD7CAYAAABjVUMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHw9JREFUeJzt3X2UXHWd5/H3t6qr+vkx3XnuJBAiCRAeQ0ATMLI4KwwKosiAo8vuONlVhnPQdR6O7DjuoDji0WHZEQdWWXdW9Owc8SErQUfjBsNDEppMIIbEAAmEpNOh009JP1dXf/ePup10J93prkp1V6fu53VOnbr31u/W/f3S3fXJ73fv75a5OyIiEj6RXFdARERyQwEgIhJSCgARkZBSAIiIhJQCQEQkpBQAIiIhpQAQEQkpBYCISEgpAEREQqog1xU4ndraWl+0aFGuqyEiclZ56aWXjrh73XjlpnUALFq0iIaGhlxXQ0TkrGJmb02knIaARERCSgEgIhJSCgARkZBSAIiIhJQCQEQkpBQAIiIhpQAQEQmpvAyA3U1H+fovd9PW1Z/rqoiITFt5GQBvHunmW//vDRo7enJdFRGRaSsvA6C6JAZAW1cixzUREZm+MgoAM3vQzLaY2SPjlFtiZrvM7KKTtt9vZk9mcuyJqC6NA9DWrSEgEZGxpB0AZrYciLr7VUCTma0ao1wUuBd4imH3HDKzC4AEEM2oxhNQFfQA2hUAIiJjyqQHsBpYb2ZPAE8H66dw96S73w10nvTS54GvZ3DcCasuSfUAWjUEJCIypnHvBmpm9wK3DNv0K6CDVHi0AzMmejAzux1Y5+49ZjZWmbXAWoAFCxZM9K1HiEUjlBcWaAhIROQ0xu0BuPtD7r5m6EHqQ7/S3e8AqoL1iboauMXMvgdcbman9ATc/TF3X+HuK+rqxr2d9ZiqSmMaAhIROY1Mvg/gReB2YANwA/DCRHd0988OLZvZT939zzM4/oTUlMRp7dYQkIjIWNI+B+DuW4G4mW0CFpIKgtNJBo+T9aV77HRUlcTVAxAROY2MvhHM3e85eZuZFQI7gWXunhhW9v4x3uP2TI49UdUlMfYeOfn8s4iIDMnaRDB37wNWDv/wz6Xq0rgmgomInEZWZwK7e2s23+9MVJfE6ewboH9gMNdVERGZlvLyVhBw4nYQ7T06DyAiMpr8DYDgdhDtuhJIRGRU+RsAx2cDqwcgIjKavA0A3Q9IROT08jYAao7fEVRDQCIio8nbANAQkIjI6eVtABTFohTFIhoCEhEZQ94GAKTuB6QhIBGR0eV1AFSVxPXF8CIiY8jrAKgujek7AURExpDfAVAS10QwEZEx5H0AtKoHICIyqjwPgBgdPQmSg57rqoiITDv5HQClcdzhaI+GgURETpbfATA0GUzDQCIip8jrAND9gERExpbXAXD8fkD6ZjARkVPkdQBoCEhEZGx5HQAaAhIRGVteB0BZYQGxqOl+QCIio8jrADAzqkri6gGIiIwirwMAUpPB9J0AIiKnyigAzOxBM9tiZo+MU26Jme0ys4uGbfuQmT1vZhvN7PxMjp+Oat0SWkRkVGkHgJktB6LufhXQZGarxigXBe4FngIKgm3zgFuBa919jbv/PuOaT1C1hoBEREaVSQ9gNbDezJ4Ang7WT+HuSXe/G+gctvlO4CDwjJl9JYNjp626NEar5gGIiJxi3AAws3uD4ZqNZrYRqAE6gn3bgRlpHO8coMLdVwEDZvYHoxxvrZk1mFlDc3NzGm89uqEegLtuCCciMty4AeDuDwXDNWvcfQ2pD/1Kd78DqArWJ6oTeDJYXgdcMsrxHnP3Fe6+oq6uLo23Hl11SZyBQaezb+CM30tEJJ9kMgT0InBjsHxDsD5Rm4FrguVrgFczOH5ahiaD6XYQIiIjpR0A7r4ViJvZJmAhsGGcXZLBA+AnwOJg3/OB9ekeP13H7wekE8EiIiMUZLKTu99z8jYzKwR2AsvcPTGs7P3Dlh24K5NjZqo6CICWrr6pPKyIyLSXtYlg7t4HrBz+4T8dzKksAuBQR2+OayIiMr1kdSawu7dm8/2yYWZ5EdGIcahdASAiMlze3woiGjFmlRfS2NGT66qIiEwreR8AAHOqitUDEBE5STgCoLKIQ+oBiIiMEIoAmFtVzKGOXs0GFhEZJhQBMKeyiL6BQd0WWkRkmJAEQDGgS0FFRIYLRQDMrUrNBWhs13kAEZEhoQgA9QBERE4VigCYURonHo1oLoCIyDChCIBIxJhdWaS5ACIiw4QiAEBzAUREThaaAJhbVUyjegAiIseFJgDmVBZx+GgvyUFNBhMRgTAFQFUxA4POkU59L4CICIQoAOZWai6AiMhwoQkAzQUQERkpNAGg2cAiIiOFJgAqi2MUx6LqAYiIBEITAGbGnCrNBRARGRKaAACYW6m5ACIiQ0IVAJoNLCJyQkYBYGYPmtkWM3tknHJLzGyXmV0UrM8ys1+a2UYz+6mZVWRy/EzNqSrmnWN9JJKDU3lYEZFpKe0AMLPlQNTdrwKazGzVGOWiwL3AU0BBsPlTwAPuvgb4MfDhTCqdqbmVRbhDk04Ei4hk1ANYDaw3syeAp4P1U7h70t3vBjqHbd4EvM/MSoE1wHMZHD9jc6o0F0BEZMi4AWBm9wZDNhvNbCNQA3QE+7YDM9I43gtAKXAfsAt4Y5TjrTWzBjNraG5uTuOtxzc0G1jnAUREJhAA7v6Qu68ZepD60K909zuAqmB9oh4AvuXuXwA2AH8xyvEec/cV7r6irq4ujbceX31NCWaw70hXVt9XRORslMkQ0IvAjcHyDcH6RC0AhsZfuoDzMjh+xopiUeZXF/P6O53jFxYRyXMF4xcZyd23mtknzGwTsAf48ji7JIMHQdlHzayNVO/hP6d7/DN1Xl0ZbzSrByAiknYAALj7PSdvM7NCYCewzN0Tw8reP2x5B3BzJsfMlsV1ZTz/RgvJQScasVxWRUQkp7I2Eczd+4CVwz/8p6PzZpbRNzCom8KJSOhldSawu7dm8/0mw+KZZQA6DyAioReqW0FA6hwAKABEREIXANWlcWpK47zRrAAQkXALXQBAqhegHoCIhF0oA2DxzFL1AEQk9MIZAHVltHUnaOnsy3VVRERyJpQBcF5wJZAmhIlImIUyABbrSiARkXAGwLyqYopiEZ0HEJFQC2UARCLGubW6EkhEwi2UAQCp8wDqAYhImIU2ABbXlXGwvYee/uT4hUVE8lBoA+C8mWW4o16AiIRWaANg8cxSQAEgIuEV2gA4p7aUaMTYc/hYrqsiIpIToQ2AwoIoS2eX88qBjlxXRUQkJ0IbAACX1Fex/e12Bgc911UREZlyoQ6AS+dXcax3gH0tuiWEiIRPqAPgkvoqAF5+uz3HNRERmXqhDoDzZpZREo8qAEQklEIdANGIsXxeJdt1IlhEQijUAQBw6YIqdjUepW9AM4JFJFwUAPOr6E8OsvuQ5gOISLhkFABm9qCZbTGzR05Tpt7M1pvZRjN73MxsovtOpeMngg/oPICIhEvaAWBmy4Gou18FNJnZqjGKtgG3ufsaoBFYlca+U2ZOZRF15YVs14lgEQmZTHoAq4H1ZvYE8HSwfgp373T3oQvsO4GOie47lcyMS+ZXKQBEJHTGDQAzuzcYxtloZhuBGlIf5hGgHZgxzv5VQL2775jIvma21swazKyhubk53fZk5NL6SvY2d9HRk5iS44mITAfjBoC7P+Tua4YepD64K939DqAqWB+VmcWBB4AvBpvG3dfdH3P3Fe6+oq6uLu0GZeLS+moAduhyUBEJkUyGgF4EbgyWbwjWT2FmMeBh4Bvu3pLOvlNt+fxKALa/3ZbjmoiITJ20A8DdtwJxM9sELAQ2jFH0PuB64LvB8NFH09h3SlUWx3jXrDK27GvNdVVERKZMQSY7ufs9J28zs0JgJ7DM3RPu/iXgSxPZdzpYfV4dT2x5i95EkqJYNNfVERGZdFmbCObufcBKdz8rz6Res6SWvoFBGt7UMJCIhENWZwK7+1k7hnLVuTXEosam16bmyiMRkVwL/a0ghpTEC7hiYTWbXjuS66qIiEwJBcAw1yyp49VDR2k+1pfrqoiITDoFwDDXLKkF4Pk31AsQkfynABjmwrmVVJfE+O0eBYCI5D8FwDDRiPGe82p59vVm3PVF8SKS3xQAJ7l2SS2Hj/bx2judua6KiMikUgCcZPWS1P2HdDWQiOQ7BcBJ5lUVs7iulI2/fyfXVRERmVQKgFH82wtn8/wbLbR06nJQEclfCoBR3HTxXJKDzi92NuW6KiIik0YBMIplc8o5t66Un798KNdVERGZNAqAUZgZN108ly37WnjnWG+uqyMiMikUAGP44MVzGHR4eoeGgUQkPykAxrBkVjnnzyrn56805roqIiKTQgFwGjddPIcX32zjUEdPrqsiIpJ1CoDTuOmSuQA89YpOBotI/lEAnMY5taVcOLeCdS9rGEhE8o8CYBy3XTGfVw508MqB9lxXRUQkqxQA47j1ivmUxKN8f/Nbua6KiEhWKQDGUVEU45bL5vGz7Y20d/fnujoiIlmjAJiAT1y9kL6BQX700oFcV0VEJGsUABOwbE4FVy6q5vub32JwUF8UIyL5IaMAMLMHzWyLmT1ymjL1ZrbezDaa2eOWcsq2zKs+tf746oW82dLNs6/rewJEJD+kHQBmthyIuvtVQJOZrRqjaBtwm7uvARqBVWNsOyt84KLZ1JbF+acXdDJYRPJDJj2A1cB6M3sCeDpYP4W7d7p7V7DaCXSMti2D4+dEYUGUO1cuYMPuw+w5fCzX1REROWPjBoCZ3RsM2Ww0s41ADakP7gjQDswYZ/8qoN7dd5xu27DX1ppZg5k1NDc3p9eaSfbvV51DSSzKwxtey3VVRETO2LgB4O4PufuaoQepD/1Kd78DqArWR2VmceAB4Iun23bS8R5z9xXuvqKuri6txky26tI4d61axFM7DvGaegEicpbLZAjoReDGYPmGYP0UZhYDHga+4e4tY20723xq9bmpXsBvXs91VUREzkjaAeDuW4G4mW0CFgIbxih6H3A98N1g+OijY2w7q1SXxvl371nEz19p5PV31AsQkbOXuWfnunYzKwR2AsvcPZGN91yxYoU3NDRk462yqrWrn9Vf+w3XL5vFw3dcluvqiIiMYGYvufuK8cplbSKYu/cBK7P14T+d1QS9gP/7SiM7Dpw1FzKJiIyQ1ZnA7t6azfebzj69ZjEzSgv5m3W/0+xgETkr6VYQGaooivGXHzifbfvb+cm/Hsx1dURE0qYAOAMfuXw+ly2o4qtP7+ZYb96PfIlInlEAnIFIxPjSBy+kpatPk8NE5KyjADhDl9RX8bEr6vmfz73Jq41Hc10dEZEJUwBkwV/dsJTq0jif++ft9A0kc10dEZEJUQBkQXVpnK99ZDm7m47x0K81FCQiZwcFQJZct3QWf3RlPY8+8wYNb4bmalgROYspALLov9x0AXOrivncP79MZ99ArqsjInJaCoAsKiss4Bu3XcKBtm7+4kcvk63bbIiITAYFQJZdde4M/vIDS1m/o4lHf7s319URERmTAmASrL32XP7w4jk8+IvdPPuavkNYRKYnBcAkMDMe/MjFLJlZzj0/3Mb+lu5cV0lE5BQKgElSWljAo5+4gkGHTz6+hSOdfbmukojICAqASbSotpTH77qSpqO9/Ifvvagrg0RkWlEATLIrFlbzrTsvZ2fjUT79/ZfoHxjMdZVERAAFwJT4N8tm8Xe3LmfTa0e4+wfbdLsIEZkWFABT5LYV9fztzRfyq1cP85/+90v0JhQCIpJbCoAp9Ml3L+Krty5n455m/vSfGujpVwiISO4oAKbYHSsX8OBHLubZ14/w8e9spkVXB4lIjigAcuC2FfU8EpwYvvXbz7O3uTPXVRKREFIA5MgNy+fww7VX09k7wK3ffp7Ne1tyXSURCZmMAsDMHjSzLWb2yGnK1JvZejPbaGaPm5kNe+1+M3syk2Pnk8sXVPOTz6yipjTOx7+zhe9s2qsbyInIlEk7AMxsORB196uAJjNbNUbRNuA2d18DNAKrgv0vABJANKMa55kFM0r46d2ruH7ZTL781C7u/sE2fcG8iEyJTHoAq4H1ZvYE8HSwfgp373T3rmC1E+gIlj8PfD2D4+atiqIY//jHV/CFG5fyy52H+eB/f5Zt+9tyXS0RyXPjBoCZ3RsM42w0s41ADakP8wjQDswYZ/8qoN7dd5jZ7cA6d+85Tfm1ZtZgZg3Nzc3ptOWsZmasvXYxP/zTq0kkndv+8QW++as9JJKaOSwik8PSHXM2s7uB3e6+wcyuBN7v7g+MUTYOPAT8tbu3mNnfA9XBy9cB/8fd/3ysY61YscIbGhrSql8+ONqb4EvrdvLjbQdZPq+Sr966nIvmVea6WiJyljCzl9x9xXjlMhkCehG4MVi+IVgfrQIx4GHgG+7eAuDun3X3u9z9LmDb6T78w6yiKMY3P3Yp3/745Rzq6OXmbz3HV556le5+3UxORLIn7QBw961A3Mw2AQuBDWMUvQ+4HvhuMHz00ZNe1wyocdywfA4bPvdePrZiPv9j0z7e/83f8vNXGnWlkIhkRdpDQGO+kVkhsBNY5u5ZuYwlrENAo9m6r5W/WbeTXYeOcuWiar5404Usn69hIRE51WQOAY3K3fuAldn68JeRVp5Tw8/vWc0DH17O3uYuPvgPz/JnP9imWcQikrGs9QAmg3oAozvam+CxZ/by3Wf30Z8c5COXz+PP3reEBTNKcl01EZkGJtoDUACcxZqP9fHIxtd5YvN+ku586JK5fGbNYpbMKs911UQkhxQAIdLU0ct3Nu3liS376UkkuW7pTP5k9Tm8Z/EMht2BQ0RCQgEQQq1d/fyv59/k+5vfoqWrn6Wzy/nkuxdx86VzKS0syHX1RGSKKABCrDeRZN32Rh5/bh+7m45RVljAhy+bxx+trOfCubpySCTfKQAEd2fb/ja+v3k/T+04RP/AIBfMqeCjV8zn5kvnMqOsMNdVFJFJoACQEdq6+ln3ciM/eukAOw52EI0Yq8+r5eZL5/L+C2ZRXhTLdRVFJEsUADKm3U1H+dn2RtZtb+Rgew/xggjXLqnlAxfN4f3LZlFZojAQOZspAGRcg4OpIaL1O5r4xe8O0djRSzRirFxUw/UXzOL6ZTNZOKM019UUkTQpACQt7s7LBzr4l51N/HrXYfYcTs0wPqe2lPe+q473nl/HVefUUBLX1UQi050CQM7IWy1d/Gb3Ozyzp5kX3mihb2CQeDTC5QurWLW4lncvnsHF86uIF+hrpUWmGwWAZE1vIsnWfa089/oRnn39CDsbjwJQHIuyYlE1Vy6qYcWiai6rr6Y4rm/6FMm1iQaA+vMyrqJYlGvfVce176oDUhPOtu5rYfPeVjbvbeHvf70HdyiIGBfMreDyBdVcvrCay+qrmF9drNnIItOUegByxjq6E7y0v5UX32xj21ttvHKgg55EEoCa0jiXzK9k+fwqls+rZPm8SmZVFCoURCaRegAyZSpLYly3dBbXLZ0FwEBykN1Nx3j5QDsvv93O9rfbeWZPM4PB/zVqy+Ism1PBBXMruGBOBUtnV3BuXSmxqM4niEwl9QBkSnT3D7Dr0FF2HOhgZ+NRXj10lD2Hj5FIpn7/YlFjcV0Z588u512zylkys4wls8qpry6mQMEgkhb1AGRaKYkXcMXCGq5YWHN8W//AIHuPdLL70DF2NR1lT9MxGt5s42fbG4+XiUcjnFNbyuKZpZxbW8a5daWcU5ta1oQ1kTOjAJCciRdEWDo7NQR0C/OObz/Wm+D1dzpHPHYdOsYvdx4mOXiix1pTGmfhjBIWzShlQU0JC2eUsKAm9agr13kGkfEoAGTaKS+KcdmCai5bUD1ie//AIPtbu9h3pJs3j3Sx90gXb7V0sXVfKz/dfpDho5mFBRHmVxdTX1PC/Opi5leXMK+qmHnVxcyrKqaurJBIRAEh4aYAkLNGvCDCeTPLOW/mqd941jeQ5EBbD/tbu9nf0s3brd0caOvh7bZu/nV/Ox09I7+qOhY1ZlUUMbeymDlVRcypLGZOZRGzKoqOP9eWxXX+QfKaAkDyQmFBlMV1ZSyuKxv19c6+AQ629XCwvZuD7b0cbOvhUEcPhzp62ba/jaaOQ8dPSA+JGNSWFTKrooiZ5YXMrCikrjxYLi+krryQ2rLUc1FME+Dk7KMAkFAoKyzg/NnlnD979O9LHhx0Wrv7aeropamjl8PHejnc0UvT0V7eOdZHY0cv299up7W7n9EunCsvLKC2vJAZpXFqywqZUXbiuaY0zozS1HJ1SZzqkph6FjItKABEgEjEqC1L/Y/+onljf2taIjlIS2c/7xzr5UhnH0eODS33p9Y7+3ijuZOtb/bTNkZYAFQWx6gpTYVBdUmcqiAYqkvjVJXEqCpOrVeWxKgqiVNZHKM0HtWJbcmqjALAzB4E3gu85O6fGaNMPfAoUALsBf7E3d3MPgT8FdAP/Ed3/31GNRfJgVg0wuzKImZXFo1bdiA5SFt3gtauflq6+mjt6qetq5+Wrv7UcneCtq5+DnX0suvQUdq6E8dnUI+mIGJUFqdCobJ45KOiKEZFcQEVRTHKRywXUFGcei4s0DCVjJR2AJjZciDq7leZ2RfNbJW7PzdK0TbgNnfvMrMvA6vMbB9wK3Ctuw+cWdVFpreCaIS64FwBjD70dLLeRJKOngRt3f20dSXo6EnQ0ZMKi46eBO3dCY72pJZbOvvZ29zFsd4ER3sHRlwiO5p4NEJ5UUHwiFFWmFouKyqgvDD1XFYYo6wwSllRAaXxAsoKCygNHqnlKKXxAl1BlScy6QGsBtab2RPAQ8B1wCkB4O6dw1Y7gQ7gTuAg8IyZbXT3+zI4vkjeKopFKYpFmVUxfg9jOHenqz/Jsd4gNLoTHOsd4Fhf8Nw7wNHeE8udvQk6+wbY39pNZ98AnX2p7eOFyJDiWDQIhlQglBZGKRn2XBIP1uNRiuMntqWWU4/iWMGJ5aBMVMEypcYNADO7F7hl2KZfkfowjwDtwIxx9q8C6t19h5l9Gki6+yoz+69m9gfu/i8nlV8LrAVYsGBBWo0RCSszoyz4X/qcyuKM3sPd6RsYTAVE3wBdQTB09g7Q1X9iW1dfMvXcP0B3f/J4ufbufg62p9a7+5P09CfpTw6mVYd4NEJRLEJJvIDieCoMU2ERpSgWoSg2tBw9/npRLEJRwYnlodcLg/Kp11LLhQWR4yGrsJlAALj7Q6T+pw+Amd0NVLr7HWZ2JakQGJWZxYEHgL8ONnUC64PldaR6DyMCwN0fAx6D1L2AJtwSETkjZnb8wzE1bHXmEslBuvuSdCdOhEJ3f5Lu/gF6+pP0JJLHt59YHqAnkaQnMUhP/wC9iUF6EklauvrpSySD15L0JpL0JtILmOEKInY8FIaCIV4QofCkbanl4LWCCIWxCIXRVLl4NLV+4vlEufjQI3rq+vDlXJ7Yz2QI6EXgdmADcAPwwmiFzCwGPAx83d1bgs2bgWuAjcHzqxkcX0TOErFohMqSCJVMzn2bhnotQ2FwIhhS670DSfqGlhPJEWX7Bk6sD9/enxykL5HkaO8ARzr7U+USg/QNnNinfyDz4DlZLGojQiEWLN+5cgGfuubcrB1nNGkHgLtvNbNPmNkmYA/w5TGK3gdcDywNEu4fgCeBDwX7/g74bxnVWkSEkb2WqeTu9CdTQTAUCCOWk0FoJE+8lto+crlvYJDEsG3Hl5ODWeuFnU7WbgdtZoXATmCZuyfGKz8Ruh20iEj6Jno76KxNR3T3PmBltj78RURkcmV1Prq7t2bz/UREZPLohiQiIiGlABARCSkFgIhISCkARERCSgEgIhJSCgARkZDK2kSwyWBmzcBbaexSCxyZpOpMZ2FtN4S37Wp3uKTb7oXuXjdeoWkdAOkys4aJzH7LN2FtN4S37Wp3uExWuzUEJCISUgoAEZGQyrcAeCzXFciRsLYbwtt2tTtcJqXdeXUOQEREJi7fegAiIjJBeRMAZvagmW0xs0dyXZfJZmb1ZrbezDaa2eOWEqb2329mTwbLoWi3mX3IzJ4Pfubnh6HdZjbLzH4ZtPmnZlaR7+02syVmtsvMLgrWT2lvNv8N8iIAzGw5EHX3q4AmM1uV6zpNsjbgNndfAzQCqwlJ+83sAiABRMPyczezecCtwLXBzzxOCNoNfAp4IGjzj4HPksftNrMocC/wFFAw2u93tn/n8yIASH0ArjezJ4Cng/W85e6d7t4VrHYClxKe9n8e+HqwHJaf+53AQeAZM/sK4Wn3JuB9ZlYKrAFayeN2u3vS3e8m9TcNo/+cs/qzz5cAqAE6SLWnHZiR2+pMDTOrAuqBCkLQfjO7HVjn7j3BprD83M8BKtx9FTAAzCQc7X4BKCX1/eK7CMnv+TCj/X5n9Xc+XwKgHah09zuAqmA9r5lZHHgA+CLhaf/VwC1m9j3gclLd5TC0uxN4MlheFzyHod0PAN9y9y8AG4ACwtHuIaP9XWf1bz1fAuBF4MZg+YZgPW+ZWQx4GPiGu7cQkva7+2fd/S53vwvYBvwhIWg3sBm4Jlgeeg5DuxcAvcFyF6lhoDC0e8hof9dZ/VvPiwBw961A3Mw2AQtJ/W8hn90HXA9818w2kvpDCVP7AfpC9HP/CbA4aOf5wN8SjnZ/GXg06PF9jdRJ4TC0OwkkR/v9zvbvvCaCiYiEVF70AEREJH0KABGRkFIAiIiElAJARCSkFAAiIiGlABARCSkFgIhISCkARERC6v8DfKz8qbEbZbsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict), Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(23.6146, dtype=torch.float64)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.KLDivLoss(size_average=False)\n",
    "a = torch.tensor(np.random.normal(size = (32, 10)))\n",
    "b = torch.tensor(np.random.normal(size = ( 32, 10)))\n",
    "criterion(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.tensor(np.random.normal(size = ( 32, 10))), torch.tensor(np.random.normal(size = ( 32, 10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        print('src shape = {}'.format(src.shape))\n",
    "        print('tgt shape = {}'.format(tgt.shape))\n",
    "        yield Batch(src, tgt, 0)\n",
    "        \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "V = 11\n",
    "data = data_gen(V, 30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "            #print('ntokens = {}'.format(self.ntokens))\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        #print('tgt = {}, aaa = {}'.format(tgt_mask.shape, Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)).shape))\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        #print('{}'.format(tgt_mask.shape))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape = torch.Size([30, 10])\n",
      "tgt shape = torch.Size([30, 10])\n",
      "a = torch.Size([30, 9, 9])\n",
      "src shape = torch.Size([30, 10])\n",
      "tgt shape = torch.Size([30, 10])\n",
      "a = torch.Size([30, 9, 9])\n",
      "src shape = torch.Size([30, 10])\n",
      "tgt shape = torch.Size([30, 10])\n",
      "a = torch.Size([30, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(data):\n",
    "    d_src = d.src\n",
    "    d_tgt = d.trg,\n",
    "    d_tgt_y = d.trg_y\n",
    "    d_src_msk = d.src_mask\n",
    "    d_tgt_msk = d.trg_mask\n",
    "    print('a = {}'.format(d_tgt_msk.shape))\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out data\n",
    "out = torch.FloatTensor(np.random.uniform(size = (30, 9 , 11)))\n",
    "d_tgt_y.type_as('Float')\n",
    "sm = torch.nn.Softmax(dim = 1)\n",
    "out = sm(out)\n",
    "\n",
    "# one hot\n",
    "y = d_tgt_y.contiguous().view(-1)\n",
    "ones=torch.ones(y.size())\n",
    "y_one_hot = torch.zeros_like(out.view(-1, out.size(-1)))\n",
    "y_one_hot.scatter_(1, y.unsqueeze(-1),ones.unsqueeze(-1))\n",
    "\n",
    "# loss\n",
    "criterion = torch.nn.KLDivLoss(size_average=False)\n",
    "criterion(out.contiguous().view(-1, out.size(-1)), y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitoperation for masking observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = torch.Size([2, 1, 4]), c shape = torch.Size([1, 4, 4])\n",
      "a = tensor([[[6, 1, 9, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0]]]),\n",
      " c = tensor([[[1, 0, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]]]),\n",
      " b = tensor([[[0, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [0, 1, 1, 0],\n",
      "         [0, 1, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[6,1,9,0], [0,0,0,0]])\n",
    "a = a.unsqueeze(-2)\n",
    "c = Variable(subsequent_mask(4)).type_as(a.data)\n",
    "print('a shape = {}, c shape = {}'.format(a.shape, c.shape))\n",
    "\n",
    "b = a & c\n",
    "print('a = {},\\n c = {},\\n b = {}'.format(a, c, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 1, 0],\n",
       "         [1, 0, 1, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 1, 0, 1]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = torch.FloatTensor(np.random.normal(size = (32, 11, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = torch.nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11, 10])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(512, 10)\n",
    "res = gen.forward(x)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "ys = torch.ones(1, 1).fill_(1).type_as(src.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(subsequent_mask(ys.size(1))).type_as(src.data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(5)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# a: value, b: position of max\n",
    "a, b= torch.max(torch.tensor([[1,2,3],[4,5,6]]), dim = 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
